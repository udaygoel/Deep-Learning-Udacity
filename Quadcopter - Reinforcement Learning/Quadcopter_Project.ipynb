{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent to fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! \n",
    "\n",
    "Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Take a look at the files in the directory to better understand the structure of the project. \n",
    "\n",
    "- `task.py`: Define your task (environment) in this file.\n",
    "- `agents/`: Folder containing reinforcement learning agents.\n",
    "    - `policy_search.py`: A sample agent has been provided here.\n",
    "    - `agent.py`: Develop your agent here.\n",
    "- `physics_sim.py`: This file contains the simulator for the quadcopter.  **DO NOT MODIFY THIS FILE**.\n",
    "\n",
    "For this project, you will define your own task in `task.py`.  Although we have provided a example task to get you started, you are encouraged to change it.  Later in this notebook, you will learn more about how to amend this file.\n",
    "\n",
    "You will also design a reinforcement learning agent in `agent.py` to complete your chosen task.  \n",
    "\n",
    "You are welcome to create any additional files to help you to organize your code.  For instance, you may find it useful to define a `model.py` file defining any needed neural network architectures.\n",
    "\n",
    "## Controlling the Quadcopter\n",
    "\n",
    "We provide a sample agent in the code cell below to show you how to use the sim to control the quadcopter.  This agent is even simpler than the sample agent that you'll examine (in `agents/policy_search.py`) later in this notebook!\n",
    "\n",
    "The agent controls the quadcopter by setting the revolutions per second on each of its four rotors.  The provided agent in the `Basic_Agent` class below always selects a random action for each of the four rotors.  These four speeds are returned by the `act` method as a list of four floating-point numbers.  \n",
    "\n",
    "For this project, the agent that you will implement in `agents/agent.py` will have a far more intelligent method for selecting actions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Basic_Agent():\n",
    "    def __init__(self, task):\n",
    "        self.task = task\n",
    "    \n",
    "    def act(self):\n",
    "        new_thrust = random.gauss(450., 25.)\n",
    "        return [new_thrust + random.gauss(0., 1.) for x in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to have the agent select actions to control the quadcopter.  \n",
    "\n",
    "Feel free to change the provided values of `runtime`, `init_pose`, `init_velocities`, and `init_angle_velocities` below to change the starting conditions of the quadcopter.\n",
    "\n",
    "The `labels` list below annotates statistics that are saved while running the simulation.  All of this information is saved in a text file `data.txt` and stored in the dictionary `results`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "from task import Task\n",
    "\n",
    "# Modify the values below to give the quadcopter a different starting position.\n",
    "runtime = 5.                                     # time limit of the episode\n",
    "init_pose = np.array([0., 0., 10., 0., 0., 0.])  # initial pose\n",
    "init_velocities = np.array([0., 0., 0.])         # initial velocities\n",
    "init_angle_velocities = np.array([0., 0., 0.])   # initial angle velocities\n",
    "file_output = 'data.txt'                         # file name for saved results\n",
    "\n",
    "# Setup\n",
    "task = Task(init_pose, init_velocities, init_angle_velocities, runtime)\n",
    "agent = Basic_Agent(task)\n",
    "done = False\n",
    "labels = ['time', 'x', 'y', 'z', 'phi', 'theta', 'psi', 'x_velocity',\n",
    "          'y_velocity', 'z_velocity', 'phi_velocity', 'theta_velocity',\n",
    "          'psi_velocity', 'rotor_speed1', 'rotor_speed2', 'rotor_speed3', 'rotor_speed4']\n",
    "results = {x : [] for x in labels}\n",
    "\n",
    "# Run the simulation, and save the results.\n",
    "with open(file_output, 'w') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(labels)\n",
    "    while True:\n",
    "        rotor_speeds = agent.act()\n",
    "        _, _, done = task.step(rotor_speeds)\n",
    "        to_write = [task.sim.time] + list(task.sim.pose) + list(task.sim.v) + list(task.sim.angular_v) + list(rotor_speeds)\n",
    "        for ii in range(len(labels)):\n",
    "            results[labels[ii]].append(to_write[ii])\n",
    "        writer.writerow(to_write)\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to visualize how the position of the quadcopter evolved during the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAD9hJREFUeJzt3X2QXXV9x/H3l2zSLQgFN6tVQrthiGUCxaAbC9jaJmmL2gKCdsqTsUomEzttY+2M4PBki51BhnEE0ymTQYlMGUDQqdAqlIJaOpWQDeExQYG04CoPS9KRxy2BfPvHPWEyS7Kbvefcu9kf79fMzr333HPP7/udO/u5vz33nD2RmUiSpr99proASVIzDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIXq6Odjs2bNzYGCgm0NK0rS3fv36ZzOzf6L1uhroAwMDDA0NdXNISZr2IuLxPVnPXS6SVAgDXZIKYaBLUiG6ug9dkqbKtm3bGB4eZnR0dKpL2a3e3l7mzJnDzJkz23q9gS7pTWF4eJj999+fgYEBImKqy3mDzGTLli0MDw8zd+7ctrYx4S6XiPh6RDwTEQ/utOytEXFbRDxS3R7U1uiS1CWjo6P09fXtlWEOEBH09fXV+gtiT/ahrwE+OGbZOcDtmTkPuL16LEl7tb01zHeoW9+EgZ6Z/wFsHbP4JOAb1f1vAB+pVYUkqbZ2j3J5e2Y+CVDdvm13K0bE8ogYioihkZGRNoeTJE2k44ctZubqzBzMzMH+/gnPXJUktandQH86It4BUN0+01xJklSedevWcdRRRzE6OsqLL77IEUccwYMPPjjxCyeh3cMWbwI+AVxc3X6nsYokqcP+9uaH2Pjz5xrd5vx3HsCFJxyx2+cXLlzIiSeeyHnnncfLL7/MmWeeyZFHHtloDRMGekRcC/weMDsihoELaQX5NyPiLOAJ4E8arUqSCnTBBRewcOFCent7ufzyyxvf/oSBnpmn7eapJQ3XIkldMd5MupO2bt3KCy+8wLZt2xgdHWW//fZrdPv+LxdJ6pLly5dz0UUXccYZZ3D22Wc3vn1P/ZekLrj66qvp6enh9NNP57XXXuO4447jjjvuYPHixY2NYaBLUhcsXbqUpUuXAjBjxgzWrl3b+BjucpGkQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpC44//zzueyyy15/fO655zb+D7o8U1TSm8/3zoGnHmh2m7/6m/Chi3f79FlnncUpp5zCypUr2b59O9dddx133313oyUY6JLUBQMDA/T19bFhwwaefvppjj76aPr6+hodw0CX9OYzzky6k5YtW8aaNWt46qmn+NSnPtX49t2HLkldcvLJJ3PLLbewbt06jj/++Ma37wxdkrpk1qxZLFq0iAMPPJAZM2Y0vn0DXZK6ZPv27dx1113ccMMNHdm+u1wkqQs2btzIYYcdxpIlS5g3b15HxnCGLkldMH/+fDZv3tzRMZyhS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JLUBVdccQULFixgwYIFzJ07l0WLFjU+hsehS3rT+dLdX+LhrQ83us3D33o4Z7/v7N0+v2LFClasWMG2bdtYvHgxn/3sZxsdH5yhS1JXrVy5ksWLF3PCCSc0vm1n6JLedMabSXfSmjVrePzxx1m1alVHtl8r0CPir4FlQAIPAJ/MzNEmCpOkkqxfv55LL72UO++8k3326czOkba3GhEHA38FDGbmkcAM4NSmCpOkkqxatYqtW7eyaNEiFixYwLJlyxofo+4ulx7glyNiG7Av8PP6JUlSea666qqOj9H2DD0zfwZcCjwBPAn8IjP/ranCJEmTU2eXy0HAScBc4J3AfhFx5i7WWx4RQxExNDIy0n6lkqRx1dkz//vAf2fmSGZuA74NHDd2pcxcnZmDmTnY399fYzhJqiczp7qEcdWtr06gPwEcExH7RkQAS4BNtaqRpA7p7e1ly5Yte22oZyZbtmyht7e37W20/aVoZq6NiBuBe4BXgQ3A6rYrkaQOmjNnDsPDw+zNu357e3uZM2dO26+vdZRLZl4IXFhnG5LUDTNnzmTu3LlTXUZHeeq/JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFqBXoEXFgRNwYEQ9HxKaIOLapwiRJk9NT8/WXAbdk5sciYhawbwM1SZLa0HagR8QBwAeAPwPIzFeAV5opS5I0WXV2uRwKjABXRcSGiLgyIvYbu1JELI+IoYgYGhkZqTGcJGk8dQK9B3gP8I+ZeTTwInDO2JUyc3VmDmbmYH9/f43hJEnjqRPow8BwZq6tHt9IK+AlSVOg7UDPzKeAn0bEb1SLlgAbG6lKkjRpdY9y+UvgmuoIl83AJ+uXJElqR61Az8x7gcGGapEk1eCZopJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQtQM9ImZExIaI+JcmCpIktaeJGfpKYFMD25Ek1VAr0CNiDvBHwJXNlCNJalfdGfpXgM8B2xuoRZJUQ9uBHhF/DDyTmesnWG95RAxFxNDIyEi7w0mSJlBnhv5+4MSI+B/gOmBxRPzT2JUyc3VmDmbmYH9/f43hJEnjaTvQM/PzmTknMweAU4E7MvPMxiqTJE2Kx6FLUiF6mthIZv4A+EET25IktccZuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhWg70CPikIj4fkRsioiHImJlk4VJkianp8ZrXwX+JjPviYj9gfURcVtmbmyoNknSJLQ9Q8/MJzPznur+88Am4OCmCpMkTU4j+9AjYgA4Gli7i+eWR8RQRAyNjIw0MZwkaRdqB3pEvAX4FvCZzHxu7POZuTozBzNzsL+/v+5wkqTdqBXoETGTVphfk5nfbqYkSVI76hzlEsDXgE2Z+eXmSpIktaPODP39wMeBxRFxb/Xz4YbqkiRNUtuHLWbmfwLRYC2SpBo8U1SSCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVwkCXpEIY6JJUCANdkgphoEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmFMNAlqRAGuiQVolagR8QHI+LHEfFoRJzTVFGSpMlrO9AjYgbwD8CHgPnAaRExv6nCJEmTU2eG/j7g0czcnJmvANcBJzVTliRpsuoE+sHAT3d6PFwtkyRNgTqBHrtYlm9YKWJ5RAxFxNDIyEiN4SRJ46kT6MPAITs9ngP8fOxKmbk6Mwczc7C/v7/GcJKk8dQJ9HXAvIiYGxGzgFOBm5opS5I0WT3tvjAzX42IvwBuBWYAX8/MhxqrTJI0KW0HOkBmfhf4bkO1SJJq8ExRSSqEgS5JhTDQJakQBrokFcJAl6RCROYbTu7s3GARI8Djbb58NvBsg+VMpVJ6KaUPsJe9VSm91O3j1zNzwjMzuxrodUTEUGYOTnUdTSill1L6AHvZW5XSS7f6cJeLJBXCQJekQkynQF891QU0qJReSukD7GVvVUovXelj2uxDlySNbzrN0CVJ45iSQJ/o4tIR8UsRcX31/NqIGKiWD0TEyxFxb/VzRbV8/52W3RsRz0bEV6ZjL9Vzp0XEAxFxf0TcEhGzp3Evf1r18VBEXNKNPur0Uj13VET8qKr5gYjorZa/t3r8aERcHhG7usjLdOjj7yPipxHxQqfrH1Nro71ExL4R8a8R8XC1/OLp2ku1/JaIuK9afkW0rts8OZnZ1R9a/2r3MeBQYBZwHzB/zDp/DlxR3T8VuL66PwA8uAdjrAc+MB17ofUfMJ8BZlePLwG+ME176QOeAPqrx98AluzlvfQA9wPv3qmHGdX9u4FjaV2t63vAh6ZpH8cA7wBe6PR70clegH2BRdWyWcCdnX5POvy+HFDdBvAt4NTJ1jYVM/Q9ubj0SbR++QFuBJbs6WwoIuYBb6P15nZaJ3qJ6me/ar0D2MWVoDqgE70cCvwkM3dce/DfgY82WPPu1OnlD4H7M/M+gMzckpmvRcQ7aP3C/Shbv3VXAx+Zbn1U9+/KzCc7XPtYjfeSmS9l5verZa8A99C6ctq066W6/1y1fg+tD4pJf8E5FYG+JxeXfn2dzHwV+AWtTzKAuRGxISJ+GBG/s4vtn0br07Ab3/Y23ktmbgM+DTxAK8jnA1/rWAe7qLPSxPvyKHB4tUumh1YAHkLn1enlXUBGxK0RcU9EfG6n9Ycn2GbTOtHHVOloLxFxIHACcHsHah+rY71ExK20/kJ/ntYHwaTUusBFm/bk4tK7W+dJ4Ncyc0tEvBf454g4YqdPNmj9efPxZkqdUOO9AC/TCvSjgc3AV4HPA19srOpd68T78r8R8WngemA78F+0Zu2dVqeXHuC3gYXAS8DtEbEeeG4363dS431kZjcCb1c61ks1WbgWuDwzNzdX8m51rJfMPL7ap34NsBi4bTKFTcUMfU8uLv36OtWb9SvA1sz8v8zcApCZ62ntx3rXjhdFxLuBnuq5buhELwuqZY9Vf2V8Eziuk02MrbPSyPuSmTdn5m9l5rHAj4FHOtrFmDore9xLtfyHmflsZr5E64pc76mW7/zn/C4vit6wTvQxVTrZy2rgkczsyoEQdPh9ycxRWtdnHrsbZ0JTEeh7cnHpm4BPVPc/BtyRmRkR/Tu++Y2IQ4F5tGaxO5xG65O6WzrRy8+A+RGx4x/x/AGwqcN9QIfel4h4W3V7EK0viq7seCc1eqF1jdyjqiMoeoDfBTZW+5yfj4hjqn2hS4HvTLc+OlzveDrSS0R8kVZYfqYLPezQeC8R8Zbqe5odHwAfBh6edGWT/Ra1iZ+q2J/QmsmdWy37O+DE6n4vcAOtfbB3A4dWyz8KPETrW+V7gBPGbHczcPh07wVYQSvE7wduBvqmcS/X0vrl20gb39p3u5fquTOrfh4ELtlp+WC17DFgFdWJedOwj0tozRS3V7dfmI7vCa2ZcVa/K/dWP8umaS9vp/VBcX/13Fdp7W2YVF2eKSpJhfBMUUkqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1Ih/h8rPRC2/4JwiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81cc2a55c0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results['time'], results['x'], label='x')\n",
    "plt.plot(results['time'], results['y'], label='y')\n",
    "plt.plot(results['time'], results['z'], label='z')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next code cell visualizes the velocity of the quadcopter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b91bc470>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results['time'], results['x_velocity'], label='x_hat')\n",
    "plt.plot(results['time'], results['y_velocity'], label='y_hat')\n",
    "plt.plot(results['time'], results['z_velocity'], label='z_hat')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you can plot the Euler angles (the rotation of the quadcopter over the $x$-, $y$-, and $z$-axes),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEm9JREFUeJzt3X+QXXV5x/H3QxJcQCw1WSma2IURyQ8IEVYE19ZfxSoN0NGMhRFKShXBscpYR+P0j0bHzihkqhVKY4bQwGgDFqWCVSgFIsVG0g3yY0OCCrPiFh2WpCAxRAk+/eOeYEg22bPsPXvzTd6vmTt777nfPfs8c2c/93vPj3siM5EkleOAThcgSRobg1uSCmNwS1JhDG5JKozBLUmFMbglqTAGtyQVxuCWpMIY3JJUmMlNrHTatGnZ09PTxKolaZ+0du3aJzKzu87YRoK7p6eH/v7+JlYtSfukiPhJ3bFuKpGkwhjcklQYg1uSCtPINm5JquPZZ59laGiIrVu3drqUCdPV1cX06dOZMmXKi16HwS2pY4aGhjj00EPp6ekhIjpdTuMyk40bNzI0NMSRRx75otfjphJJHbN161amTp26X4Q2QEQwderUcX/CMLglddT+EtrbtaNfg1uSCmNwS9IIenp6eOKJJ3ZZfuONN/K5z32uAxX9ljsnJWkMzjjjDM4444yO1uCMW9J+bXBwkJkzZ3Leeecxd+5cFixYwJYtWwC47LLLOOGEEzjuuOPYsGEDACtWrODDH/5wJ0t2xi1p7/Dpm9bx4GO/aOs6Z7/yZfzt6XNGHffQQw+xfPly+vr6OP/887niiisAmDZtGvfccw9XXHEFS5Ys4corr2xrfS+WM25J+70ZM2bQ19cHwDnnnMNdd90FwLvf/W4ATjzxRAYHBztV3i5qzbgj4jDgSuBYIIHzM3N1k4VJ2r/UmRk3ZedD9LY/fslLXgLApEmT2LZt24TXtTt1Z9z/ANycmTOB44H1zZUkSRPr0UcfZfXq1lx05cqVvOlNb+pwRXs2anBHxMuAPwSWA2TmrzPzyaYLk6SJMmvWLK6++mrmzp3Lpk2buOiiizpd0h7V2VRyFDAM/HNEHA+sBT6amb9stDJJmiAHHHAAS5cufcGyHbdp9/b2smrVKgAWLlzIwoULJ664EdTZVDIZOAH4p8x8HfBLYNHOgyLigojoj4j+4eHhNpcpSdquTnAPAUOZeXf1+HpaQf4CmbksM3szs7e7u9Zl0ySp43p6ehgYGOh0GWMyanBn5s+Bn0bEMdWitwMPNlqVJGm36p6A81fAVyPiQOAR4C+aK0mStCe1gjsz7wV6G65FklSDZ05KUmEMbkn7rSeffPL57yVZtWoV8+fPH9Pvr1ixgscee6yJ0vbI4Ja039oxuF+MTgW33w4oab+1aNEiHn74YebNm8eUKVM45JBDWLBgAQMDA5x44ol85StfISJYu3YtH/vYx9i8eTPTpk1jxYoVfO9736O/v5/3ve99HHTQQaxevZpLL72Um266iWeeeYY3vvGNfPnLX27k0myRmW1faW9vb/b397d9vZL2LevXr2fWrFmtB99ZBD9/oL1/4PeOg3ft/mo1g4ODzJ8/n4GBAVatWsWZZ57JunXreOUrX0lfXx+XXnopb3jDG3jzm9/MN7/5Tbq7u7nuuuu45ZZbuOqqq3jLW97CkiVL6O1tHbuxadMmXv7ylwNw7rnn8t73vpfTTz99z31XImJtZtY6CMQZtyRVTjrpJKZPnw7AvHnzGBwc5LDDDmNgYIBTTz0VgOeee44jjjhixN+/4447uOSSS9iyZQubNm1izpw5Iwb3eBnckvYOe5gZT5TtX+MKv/0q18xkzpw5z3974O5s3bqVD33oQ/T39zNjxgwWL17M1q1bG6nTnZOS9luHHnooTz/99B7HHHPMMQwPDz8f3M8++yzr1q3b5fe3h/S0adPYvHkz119/fWN1O+OWtN+aOnUqfX19HHvssRx00EEcfvjhu4w58MADuf766/nIRz7CU089xbZt27j44ouZM2cOCxcu5MILL3x+5+QHPvABjjvuOHp6enj961/fWN3unJTUMSPtpNsfjHfnpJtKJKkwBrckFcbglqTCGNySVBiDW5IKY3BLUmEMbkmqaenSpVxzzTWdLsMTcCSprgsvvLDTJQDOuCXt5wYHB5k5cybnnXcec+fOZcGCBWzZsoVFixYxe/Zs5s6dy8c//nEAFi9ezJIlSzpcsTNuSXuJz6/5PBs2bWjrOme+fCafPOmTo4576KGHWL58OX19fZx//vlcfvnl3HDDDWzYsIGI4Mknn2xrXePljFvSfm/GjBn09fUBcM4553DnnXfS1dXF+9//fr7xjW9w8MEHd7jCF3LGLWmvUGdm3JSdr1IzZcoU1qxZw2233ca1117L5Zdfzu23396h6nZlcEva7z366KOsXr2aU045hZUrVzJv3jyeeuopTjvtNE4++WRe85rXdLrEF6gV3BExCDwNPAdsq/sNVpJUglmzZnH11VfzwQ9+kKOPPprFixczf/58tm7dSmbyhS98odMlvsBYZtxvzcwnGqtEkjrkgAMOYOnSpS9YtmbNml3GLV68eIIq2jN3TkpSYeoGdwL/ERFrI+KCJguSpInU09PDwMBAp8sYk7qbSvoy87GIeAVwa0RsyMw7dxxQBfoFAK9+9avbXKakfVVm7nJUx76sHVcdqzXjzszHqp+PAzcAJ40wZllm9mZmb3d397gLk7Tv6+rqYuPGjW0JsxJkJhs3bqSrq2tc6xl1xh0RhwAHZObT1f13AJ8Z11+VJGD69OkMDQ0xPDzc6VImTFdXF9OnTx/XOupsKjkcuKH6KDMZ+JfMvHlcf1WSaJ3ocuSRR3a6jOKMGtyZ+Qhw/ATUIkmqwcMBJakwBrckFcbglqTCGNySVBiDW5IKY3BLUmEMbkkqjMEtSYUxuCWpMAa3JBXG4JakwhjcklQYg1uSCmNwS1JhDG5JKozBLUmFMbglqTAGtyQVxuCWpMIY3JJUGINbkgpjcEtSYWoHd0RMiogfRMS3mixIkrRnY5lxfxRY31QhkqR6agV3REwH/gS4stlyJEmjqTvj/iLwCeA3DdYiSaph1OCOiPnA45m5dpRxF0REf0T0Dw8Pt61ASdIL1Zlx9wFnRMQgcC3wtoj4ys6DMnNZZvZmZm93d3eby5QkbTdqcGfmpzJzemb2AGcBt2fmOY1XJkkakcdxS1JhJo9lcGauAlY1UokkqRZn3JJUGINbkgpjcEtSYQxuSSqMwS1JhTG4JakwBrckFcbglqTCGNySVBiDW5IKY3BLUmEMbkkqjMEtSYUxuCWpMAa3JBXG4JakwhjcklQYg1uSCmNwS1JhDG5JKozBLUmFMbglqTAGtyQVZtTgjoiuiFgTEfdFxLqI+PREFCZJGtnkGmN+BbwtMzdHxBTgroj4TmZ+v+HaJEkjGDW4MzOBzdXDKdUtmyxKkrR7tbZxR8SkiLgXeBy4NTPvbrYsSdLu1AruzHwuM+cB04GTIuLYncdExAUR0R8R/cPDw+2uU5JUGdNRJZn5JLAKeOcIzy3LzN7M7O3u7m5TeZKkndU5qqQ7Ig6r7h8E/BGwoenCJEkjq3NUyRHA1RExiVbQfy0zv9VsWZKk3alzVMn9wOsmoBZJUg2eOSlJhTG4JakwBrckFcbglqTCGNySVBiDW5IKY3BLUmEMbkkqjMEtSYUxuCWpMAa3JBXG4JakwhjcklQYg1uSCmNwS1JhDG5JKozBLUmFMbglqTAGtyQVxuCWpMIY3JJUGINbkgozanBHxIyIuCMi1kfEuoj46EQUJkka2eQaY7YBf52Z90TEocDaiLg1Mx9suDZJ0ghGnXFn5s8y857q/tPAeuBVTRcmSRrZmLZxR0QP8Drg7iaKkSSNrnZwR8RLga8DF2fmL0Z4/oKI6I+I/uHh4XbWKEnaQa3gjogptEL7q5n5jZHGZOayzOzNzN7u7u521ihJ2kGdo0oCWA6sz8y/b74kSdKe1Jlx9wHnAm+LiHur22kN1yVJ2o1RDwfMzLuAmIBaJEk1eOakJBXG4JakwhjcklQYg1uSCmNwS1JhDG5JKozBLUmFMbglqTAGtyQVxuCWpMIY3JJUGINbkgpjcEtSYQxuSSqMwS1JhTG4JakwBrckFcbglqTCGNySVBiDW5IKY3BLUmEMbkkqjMEtSYUZNbgj4qqIeDwiBiaiIEnSntWZca8A3tlwHZKkmkYN7sy8E9g0AbVIkmpo2zbuiLggIvojon94eLhdq5Uk7aRtwZ2ZyzKzNzN7u7u727VaSdJOPKpEkgpjcEtSYeocDrgSWA0cExFDEfGXzZclSdqdyaMNyMyzJ6IQSVI9biqRpMIY3JJUGINbkgpjcEtSYQxuSSqMwS1JhTG4JakwBrckFcbglqTCGNySVBiDW5IKY3BLUmEMbkkqjMEtSYUxuCWpMAa3JBXG4JakwhjcklQYg1uSCmNwS1JhDG5JKozBLUmFqRXcEfHOiHgoIn4cEYuaLkqStHujBndETAL+EXgXMBs4OyJmN12YJGlkdWbcJwE/zsxHMvPXwLXAmc2WJUnanTrB/Srgpzs8HqqWSZI6oE5wxwjLcpdBERdERH9E9A8PD4+/MknSiOoE9xAwY4fH04HHdh6Umcsyszcze7u7u9tVnyRpJ3WC+3+AoyPiyIg4EDgLuLHZsiRJuzN5tAGZuS0iPgzcAkwCrsrMdY1XJkka0ajBDZCZ3wa+3XAtkqQaPHNSkgpjcEtSYQxuSSqMwS1JhTG4JakwkbnLSZDjX2nEMPCTF/nr04An2lhOJ+0rvewrfYC97I32lT5gfL38fmbWOnuxkeAej4joz8zeTtfRDvtKL/tKH2Ave6N9pQ+YuF7cVCJJhTG4Jakwe2NwL+t0AW20r/Syr/QB9rI32lf6gAnqZa/bxi1J2rO9ccYtSdqDRoN7tIsMR8RLIuK66vm7I6KnWt4TEc9ExL3VbWm1/NAdlt0bEU9ExBeb7KGpXqrnzo6IByLi/oi4OSKmFdzLn1V9rIuIS/bmPqrn5kbE6qreByKiq1p+YvX4xxHxpYgY6UIipfTydxHx04jYPBE97FBPW3uJiIMj4t8jYkO1/HOl9lItvzki7quWL43WdX3HJjMbudH6CtiHgaOAA4H7gNk7jfkQsLS6fxZwXXW/Bxio8TfWAn/YVA9N9kLrmxkfB6ZVjy8BFhfay1TgUaC7enw18Pa9uI/JwP3A8TvUP6m6vwY4hdaVn74DvGsvf0321MvJwBHA5qZ7aLIX4GDgrdWyA4H/Kvx1eVn1M4CvA2eNtbYmZ9x1LjJ8Jq1/coDrgbfXneFExNHAK2i9iE1ropeobodU417GCFcWakATvRwF/DAzt1+z7j+B97Sx5pGMp493APdn5n0AmbkxM5+LiCNo/VOtztZ/1jXAnzbcRyO9VPe/n5k/m4D6d9T2XjJzS2beUS37NXAPrStxFddLdf8X1fjJtN4QxryjscngrnOR4efHZOY24Cla70wAR0bEDyLiuxHxByOs/2xa724TsXe17b1k5rPARcADtAJ7NrC8sQ5GqLPSjtflx8DMalPKZFphN4NmjaeP1wIZEbdExD0R8Ykdxg+Nss4mNNFLpzTaS0QcBpwO3NZA7TtrrJeIuIXWJ+6naQX+mNS6kMKLVOciw7sb8zPg1Zm5MSJOBP4tIubs8E4FrY8l57an1FG1vRfgGVrB/TrgEeAy4FPAZ9tW9ciaeF3+LyIuAq4DfgP8N61ZeJPG08dk4E3A64EtwG0RsRb4xW7GN63tvWTmRATbSBrrpZoUrAS+lJmPtK/k3Wqsl8z842qb91eBtwG3jqWwJmfcdS4y/PyY6kX5HWBTZv4qMzcCZOZaWtuZXrv9lyLieGBy9dxEaKKXedWyh6tPDV8D3thkEzvXWWnL65KZN2XmGzLzFOAh4EeNdjGOPqrl383MJzJzC62rO51QLd/xI/iIF8ZuQBO9dEqTvSwDfpSZE3JAAg2/Lpm5ldb1e3fe/DKqJoO7zkWGbwTOq+4vAG7PzIyI7u17WiPiKOBoWrPS7c6m9c47UZro5X+B2RGx/UtlTgXWN9wHNPS6RMQrqp+/S2uHzZV7ax+0rp86tzpaYTLwZuDBanvw0xFxcrWd8s+BbzbcRyO9TEDNu9NILxHxWVqhePEE9LBd23uJiJdW+1K2B/1pwIYxVzbWvZljuVVF/ZDWzOxvqmWfAc6o7ncB/0prG+ka4Khq+XuAdbT24t4DnL7Teh8BZjZZ+0T0AlxIK6zvB24Cphbcy0pa/2QP8iL2kk9kH9Vz51S9DACX7LC8t1r2MHA51UlqhfZyCa2Z32+qn4tL7IXWTDer/5V7q9v7C+3lcFpvCPdXz11Ga+vBmOryzElJKoxnTkpSYQxuSSqMwS1JhTG4JakwBrckFcbglqTCGNySVBiDW5IK8/+P7wg/KEemygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b9266908>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results['time'], results['phi'], label='phi')\n",
    "plt.plot(results['time'], results['theta'], label='theta')\n",
    "plt.plot(results['time'], results['psi'], label='psi')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "before plotting the velocities (in radians per second) corresponding to each of the Euler angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b90f0ba8>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results['time'], results['phi_velocity'], label='phi_velocity')\n",
    "plt.plot(results['time'], results['theta_velocity'], label='theta_velocity')\n",
    "plt.plot(results['time'], results['psi_velocity'], label='psi_velocity')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use the code cell below to print the agent's choice of actions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f81b913f390>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(results['time'], results['rotor_speed1'], label='Rotor 1 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed2'], label='Rotor 2 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed3'], label='Rotor 3 revolutions / second')\n",
    "plt.plot(results['time'], results['rotor_speed4'], label='Rotor 4 revolutions / second')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying a task, you will derive the environment state from the simulator.  Run the code cell below to print the values of the following variables at the end of the simulation:\n",
    "- `task.sim.pose` (the position of the quadcopter in ($x,y,z$) dimensions and the Euler angles),\n",
    "- `task.sim.v` (the velocity of the quadcopter in ($x,y,z$) dimensions), and\n",
    "- `task.sim.angular_v` (radians/second for each of the three Euler angles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -3.55116072e-07  -6.08494595e-08   1.00044047e+01   6.28309668e+00\n",
      "   3.07690428e-04   0.00000000e+00]\n",
      "[ -3.03262807e-05  -6.66341211e-06   1.46815011e-01]\n",
      "[-0.00460881  0.01379838  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# the pose, velocity, and angular velocity of the quadcopter at the end of the episode\n",
    "print(task.sim.pose)\n",
    "print(task.sim.v)\n",
    "print(task.sim.angular_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sample task in `task.py`, we use the 6-dimensional pose of the quadcopter to construct the state of the environment at each timestep.  However, when amending the task for your purposes, you are welcome to expand the size of the state vector by including the velocity information.  You can use any combination of the pose, velocity, and angular velocity - feel free to tinker here, and construct the state to suit your task.\n",
    "\n",
    "## The Task\n",
    "\n",
    "A sample task has been provided for you in `task.py`.  Open this file in a new window now. \n",
    "\n",
    "The `__init__()` method is used to initialize several variables that are needed to specify the task.  \n",
    "- The simulator is initialized as an instance of the `PhysicsSim` class (from `physics_sim.py`).  \n",
    "- Inspired by the methodology in the original DDPG paper, we make use of action repeats.  For each timestep of the agent, we step the simulation `action_repeats` timesteps.  If you are not familiar with action repeats, please read the **Results** section in [the DDPG paper](https://arxiv.org/abs/1509.02971).\n",
    "- We set the number of elements in the state vector.  For the sample task, we only work with the 6-dimensional pose information.  To set the size of the state (`state_size`), we must take action repeats into account.  \n",
    "- The environment will always have a 4-dimensional action space, with one entry for each rotor (`action_size=4`). You can set the minimum (`action_low`) and maximum (`action_high`) values of each entry here.\n",
    "- The sample task in this provided file is for the agent to reach a target position.  We specify that target position as a variable.\n",
    "\n",
    "The `reset()` method resets the simulator.  The agent should call this method every time the episode ends.  You can see an example of this in the code cell below.\n",
    "\n",
    "The `step()` method is perhaps the most important.  It accepts the agent's choice of action `rotor_speeds`, which is used to prepare the next state to pass on to the agent.  Then, the reward is computed from `get_reward()`.  The episode is considered done if the time limit has been exceeded, or the quadcopter has travelled outside of the bounds of the simulation.\n",
    "\n",
    "In the next section, you will learn how to test the performance of an agent on this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Agent\n",
    "\n",
    "The sample agent given in `agents/policy_search.py` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (`score`), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise.\n",
    "\n",
    "Run the code cell below to see how the agent performs on the sample task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode = 1000, score =  15.299 (best =  15.300), noise_scale = 3.25"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from agents.policy_search import PolicySearch_Agent\n",
    "from task import Task\n",
    "\n",
    "num_episodes = 1000\n",
    "target_pos = np.array([0., 0., 10.])\n",
    "task = Task(target_pos=target_pos)\n",
    "agent = PolicySearch_Agent(task) \n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode() # start a new episode\n",
    "    while True:\n",
    "        action = agent.act(state) \n",
    "        next_state, reward, done = task.step(action)\n",
    "        agent.step(reward, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"\\rEpisode = {:4d}, score = {:7.3f} (best = {:7.3f}), noise_scale = {}\".format(\n",
    "                i_episode, agent.score, agent.best_score, agent.noise_scale), end=\"\")  # [debug]\n",
    "            break\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This agent should perform very poorly on this task.  And that's where you come in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Define the Task, Design the Agent, and Train Your Agent!\n",
    "\n",
    "Amend `task.py` to specify a task of your choosing.  If you're unsure what kind of task to specify, you may like to teach your quadcopter to takeoff, hover in place, land softly, or reach a target pose.  \n",
    "\n",
    "After specifying your task, use the sample agent in `agents/policy_search.py` as a template to define your own agent in `agents/agent.py`.  You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode()`, etc.).\n",
    "\n",
    "Note that it is **highly unlikely** that the first agent and task that you specify will learn well.  You will likely have to tweak various hyperparameters and the reward function for your task until you arrive at reasonably good behavior.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Use the code above as inspiration to build in a mechanism to log/save the total rewards obtained in each episode to file.  If the episode rewards are gradually increasing, this is an indication that your agent is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Train your agent here.\n",
    "\n",
    "from task import Task\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Setting up the task\n",
    "# The task is to go from initial position to a target position. Initial \n",
    "# velocity and angular velocity are assumed to be zero. \n",
    "init_pose = np.array([7., 5., 10., 0.01, 0.01, 0.01])\n",
    "target_pos = np.array([5., 3., 12.])\n",
    "runtime = 5   # Making sure the agent has enough time to run the episode\n",
    "task = Task(init_pose = init_pose, target_pos = target_pos, runtime = runtime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scaler for  states\n",
      "Min value:  [-5.]\n",
      "Max value:  [ 15.]\n",
      "\n",
      "Scaler for  actions\n",
      "Min value:  [ 0.]\n",
      "Max value:  [ 900.]\n"
     ]
    }
   ],
   "source": [
    "# Setting up min max scalars for scaling states and actions to use in Neural Networks\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "states_range = [[-5], [15]]\n",
    "action_range = [[task.action_low], [task.action_high]]\n",
    "\n",
    "def get_scaler(val_range, text):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(val_range)\n",
    "    print('\\nScaler for ', text)\n",
    "    print('Min value: ', scaler.data_min_)\n",
    "    print('Max value: ', scaler.data_max_)\n",
    "    return scaler\n",
    "\n",
    "states_scaler = get_scaler(states_range, 'states')\n",
    "actions_scaler = get_scaler(action_range, 'actions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actor Local Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "states (InputLayer)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_29 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_30 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_31 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "raw_actions (Dense)          (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 137,732\n",
      "Trainable params: 136,196\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Actor Target Model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "states (InputLayer)          (None, 18)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 128)               2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "raw_actions (Dense)          (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 137,732\n",
      "Trainable params: 136,196\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "Critic Local Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actions (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_39 (Dense)                (None, 64)           1216        states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_41 (Dense)                (None, 64)           320         actions[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 64)           256         dense_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 64)           256         dense_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 64)           0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64)           0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_35 (Dropout)            (None, 64)           0           activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_37 (Dropout)            (None, 64)           0           activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_40 (Dense)                (None, 128)          8320        dropout_35[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_42 (Dense)                (None, 128)          8320        dropout_37[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 128)          512         dense_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 128)          512         dense_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 128)          0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 128)          0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 128)          0           activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_38 (Dropout)            (None, 128)          0           activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 128)          0           dropout_36[0][0]                 \n",
      "                                                                 dropout_38[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 128)          0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_43 (Dense)                (None, 64)           8256        activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_values (Dense)                (None, 1)            65          dense_43[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 28,033\n",
      "Trainable params: 27,265\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "\n",
      "Critic Target Model\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "states (InputLayer)             (None, 18)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "actions (InputLayer)            (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_44 (Dense)                (None, 64)           1216        states[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 64)           320         actions[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 64)           256         dense_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 64)           256         dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64)           0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64)           0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_39 (Dropout)            (None, 64)           0           activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_41 (Dropout)            (None, 64)           0           activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_45 (Dense)                (None, 128)          8320        dropout_39[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 128)          8320        dropout_41[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 128)          512         dense_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 128)          512         dense_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 128)          0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 128)          0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_40 (Dropout)            (None, 128)          0           activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_42 (Dropout)            (None, 128)          0           activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 128)          0           dropout_40[0][0]                 \n",
      "                                                                 dropout_42[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 128)          0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 64)           8256        activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "q_values (Dense)                (None, 1)            65          dense_48[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 28,033\n",
      "Trainable params: 27,265\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Setting parameters and creating objects\n",
    "\n",
    "num_episodes = 2000\n",
    "\n",
    "# Algorithm parameters\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "\n",
    "# Replay Buffer parameters\n",
    "buffer_size = 100000\n",
    "batch_size = 64\n",
    "\n",
    "# Ornstein - Uhlenbeck Noise parameters\n",
    "noise_mu = 0.0\n",
    "noise_theta = 0.15\n",
    "noise_sigma = 0.2*20\n",
    "\n",
    "# Actor Neural Net parameters\n",
    "actor_learning_rate = 0.001\n",
    "actor_num_hidden_units = [128, 512, 128]\n",
    "actor_dropout_rate = 0.1\n",
    "\n",
    "# Critic Neural Net parameters - parameters are some for states and actions layers\n",
    "critic_learning_rate = 0.001\n",
    "critic_num_hidden_units = [64, 128]\n",
    "critic_dropout_rate = 0.1\n",
    "\n",
    "from agents.agent import DDPG\n",
    "\n",
    "agent = DDPG(task, gamma = gamma, tau = tau, buffer_size = buffer_size, batch_size = batch_size, \n",
    "             noise_mu = noise_mu, noise_theta = noise_theta, noise_sigma = noise_sigma,\n",
    "            actor_learning_rate = actor_learning_rate, actor_num_hidden_units = actor_num_hidden_units, \n",
    "             actor_dropout_rate = actor_dropout_rate, critic_learning_rate = critic_learning_rate, \n",
    "             critic_num_hidden_units = critic_num_hidden_units, critic_dropout_rate = critic_dropout_rate,\n",
    "            states_scaler = states_scaler, actions_scaler = actions_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:     5, Score:   0.581, Best Score:   8.872, Critic Loss: 0.0528\n",
      "Episode:    10, Score:   0.766, Best Score:   8.872, Critic Loss: 0.0167\n",
      "Episode:    15, Score:   1.556, Best Score:   8.872, Critic Loss: 0.0111\n",
      "Episode:    20, Score:   0.486, Best Score:   8.872, Critic Loss: 0.0097\n",
      "Episode:    25, Score:   0.507, Best Score:   8.872, Critic Loss: 0.0127\n",
      "Episode:    30, Score:   1.306, Best Score:   8.872, Critic Loss: 0.0070\n",
      "Episode:    35, Score:   3.146, Best Score:   8.872, Critic Loss: 0.0054\n",
      "Episode:    40, Score:   3.605, Best Score:   8.872, Critic Loss: 0.0186\n",
      "Episode:    45, Score:   1.399, Best Score:   8.872, Critic Loss: 0.0117\n",
      "Episode:    50, Score:   4.634, Best Score:   8.872, Critic Loss: 0.0310\n",
      "Episode:    55, Score:   0.426, Best Score:   8.872, Critic Loss: 0.0839\n",
      "Episode:    60, Score:   4.209, Best Score:   8.872, Critic Loss: 0.0216\n",
      "Episode:    65, Score:   3.363, Best Score:   8.872, Critic Loss: 0.0706\n",
      "Episode:    70, Score:   2.145, Best Score:   8.872, Critic Loss: 0.0764\n",
      "Episode:    75, Score:   1.034, Best Score:   9.727, Critic Loss: 0.0451\n",
      "Episode:    80, Score:   1.356, Best Score:   9.727, Critic Loss: 0.0517\n",
      "Episode:    85, Score:  11.064, Best Score:  11.064, Critic Loss: 0.0648\n",
      "Episode:    90, Score:   0.710, Best Score:  11.064, Critic Loss: 0.0972\n",
      "Episode:    95, Score:   0.668, Best Score:  11.064, Critic Loss: 0.2782\n",
      "Episode:   100, Score:   0.654, Best Score:  11.064, Critic Loss: 2.2509\n",
      "Episode:   105, Score:   2.627, Best Score:  17.217, Critic Loss: 0.8946\n",
      "Episode:   110, Score:   0.917, Best Score:  17.217, Critic Loss: 4.5627\n",
      "Episode:   115, Score:   0.728, Best Score:  17.217, Critic Loss: 0.7483\n",
      "Episode:   120, Score:   0.815, Best Score:  17.217, Critic Loss: 34.7339\n",
      "Episode:   125, Score:   0.817, Best Score:  17.217, Critic Loss: 3.9195\n",
      "Episode:   130, Score:   0.677, Best Score:  17.217, Critic Loss: 5.3053\n",
      "Episode:   135, Score:   3.204, Best Score:  17.217, Critic Loss: 1229.5500\n",
      "Episode:   140, Score:   4.314, Best Score:  17.217, Critic Loss: 2.8873\n",
      "Episode:   145, Score:   3.729, Best Score:  17.217, Critic Loss: 4.9404\n",
      "Episode:   150, Score:   4.430, Best Score:  17.217, Critic Loss: 3.2210\n",
      "Episode:   155, Score:   6.839, Best Score:  17.217, Critic Loss: 849.8243\n",
      "Episode:   160, Score:   0.804, Best Score:  17.217, Critic Loss: 9.5117\n",
      "Episode:   165, Score:   4.283, Best Score:  17.217, Critic Loss: 5.1649\n",
      "Episode:   170, Score:   5.539, Best Score:  17.217, Critic Loss: 4.9447\n",
      "Episode:   175, Score:   3.595, Best Score:  17.217, Critic Loss: 684.6790\n",
      "Episode:   180, Score:   6.353, Best Score:  17.217, Critic Loss: 9.6320\n",
      "Episode:   185, Score:   0.824, Best Score:  17.217, Critic Loss: 8.0153\n",
      "Episode:   190, Score:   0.985, Best Score:  17.217, Critic Loss: 5.7064\n",
      "Episode:   195, Score:   0.795, Best Score:  17.217, Critic Loss: 5.5903\n",
      "Episode:   200, Score:   0.794, Best Score:  17.217, Critic Loss: 5.4802\n",
      "Episode:   205, Score:   0.794, Best Score:  17.217, Critic Loss: 26.3289\n",
      "Episode:   210, Score:   0.786, Best Score:  17.217, Critic Loss: 4.5613\n",
      "Episode:   215, Score:   0.802, Best Score:  17.217, Critic Loss: 9.9643\n",
      "Episode:   220, Score:   0.848, Best Score:  17.217, Critic Loss: 3.9248\n",
      "Episode:   225, Score:   3.317, Best Score:  17.217, Critic Loss: 10.3758\n",
      "Episode:   230, Score:   4.494, Best Score:  17.217, Critic Loss: 8.1259\n",
      "Episode:   235, Score:   3.972, Best Score:  17.217, Critic Loss: 14.1836\n",
      "Episode:   240, Score:   4.080, Best Score:  17.217, Critic Loss: 5.7903\n",
      "Episode:   245, Score:   3.667, Best Score:  17.217, Critic Loss: 4.7099\n",
      "Episode:   250, Score:   3.710, Best Score:  17.217, Critic Loss: 4.9534\n",
      "Episode:   255, Score:   4.290, Best Score:  17.217, Critic Loss: 432.3308\n",
      "Episode:   260, Score:   3.494, Best Score:  17.217, Critic Loss: 4.6893\n",
      "Episode:   265, Score:   0.729, Best Score:  17.217, Critic Loss: 7.0230\n",
      "Episode:   270, Score:   7.218, Best Score:  17.217, Critic Loss: 10.7787\n",
      "Episode:   275, Score:  15.693, Best Score:  17.217, Critic Loss: 4.0438\n",
      "Episode:   280, Score:   3.398, Best Score:  17.217, Critic Loss: 7.0643\n",
      "Episode:   285, Score:   4.984, Best Score:  17.217, Critic Loss: 4.9415\n",
      "Episode:   290, Score:   4.257, Best Score:  17.217, Critic Loss: 16.1999\n",
      "Episode:   295, Score:   3.649, Best Score:  17.217, Critic Loss: 10.0282\n",
      "Episode:   300, Score:   4.720, Best Score:  17.217, Critic Loss: 9.4911\n",
      "Episode:   305, Score:   3.293, Best Score:  17.217, Critic Loss: 4.0824\n",
      "Episode:   310, Score:   3.598, Best Score:  17.217, Critic Loss: 6.8780\n",
      "Episode:   315, Score:   3.643, Best Score:  17.217, Critic Loss: 24.3097\n",
      "Episode:   320, Score:   4.091, Best Score:  17.217, Critic Loss: 19.6366\n",
      "Episode:   325, Score:   3.631, Best Score:  17.217, Critic Loss: 17.1457\n",
      "Episode:   330, Score:   3.786, Best Score:  17.217, Critic Loss: 29.2462\n",
      "Episode:   335, Score:   3.700, Best Score:  17.217, Critic Loss: 4.8583\n",
      "Episode:   340, Score:   4.119, Best Score:  17.217, Critic Loss: 12.2962\n",
      "Episode:   345, Score:   3.335, Best Score:  17.217, Critic Loss: 21691.8418\n",
      "Episode:   350, Score:   3.516, Best Score:  17.217, Critic Loss: 1247.2891\n",
      "Episode:   355, Score:   1.025, Best Score:  17.217, Critic Loss: 373.6947\n",
      "Episode:   360, Score:   0.941, Best Score:  17.217, Critic Loss: 285.7336\n",
      "Episode:   365, Score:   0.929, Best Score:  17.217, Critic Loss: 759.2242\n",
      "Episode:   370, Score:   0.933, Best Score:  17.217, Critic Loss: 457.9040\n",
      "Episode:   375, Score:   0.941, Best Score:  17.217, Critic Loss: 4060.0303\n",
      "Episode:   380, Score:   0.942, Best Score:  17.217, Critic Loss: 191.4152\n",
      "Episode:   385, Score:   0.990, Best Score:  17.217, Critic Loss: 427.1183\n",
      "Episode:   390, Score:   1.005, Best Score:  17.217, Critic Loss: 274.9556\n",
      "Episode:   395, Score:   1.000, Best Score:  17.217, Critic Loss: 226.6194\n",
      "Episode:   400, Score:   0.945, Best Score:  17.217, Critic Loss: 45443.1406\n",
      "Episode:   405, Score:   1.006, Best Score:  17.217, Critic Loss: 264.5273\n",
      "Episode:   410, Score:   0.999, Best Score:  17.217, Critic Loss: 440.9142\n",
      "Episode:   415, Score:   1.055, Best Score:  17.217, Critic Loss: 1569.2131\n",
      "Episode:   420, Score:   1.021, Best Score:  17.217, Critic Loss: 551.0496\n",
      "Episode:   425, Score:   0.935, Best Score:  17.217, Critic Loss: 310.7885\n",
      "Episode:   430, Score:   0.945, Best Score:  17.217, Critic Loss: 306.4597\n",
      "Episode:   435, Score:   0.959, Best Score:  17.217, Critic Loss: 309.6276\n",
      "Episode:   440, Score:   0.943, Best Score:  17.217, Critic Loss: 305.5841\n",
      "Episode:   445, Score:   0.938, Best Score:  17.217, Critic Loss: 808.8516\n",
      "Episode:   450, Score:   0.971, Best Score:  17.217, Critic Loss: 30205.9863\n",
      "Episode:   455, Score:   1.014, Best Score:  17.217, Critic Loss: 424.6629\n",
      "Episode:   460, Score:   0.942, Best Score:  17.217, Critic Loss: 198.2054\n",
      "Episode:   465, Score:   0.965, Best Score:  17.217, Critic Loss: 183.2013\n",
      "Episode:   470, Score:   0.934, Best Score:  17.217, Critic Loss: 191.6287\n",
      "Episode:   475, Score:   0.938, Best Score:  17.217, Critic Loss: 758.0461\n",
      "Episode:   480, Score:   0.939, Best Score:  17.217, Critic Loss: 446.1757\n",
      "Episode:   485, Score:   0.936, Best Score:  17.217, Critic Loss: 246.7325\n",
      "Episode:   490, Score:   0.972, Best Score:  17.217, Critic Loss: 504.5072\n",
      "Episode:   495, Score:  13.151, Best Score:  17.217, Critic Loss: 203.5365\n",
      "Episode:   500, Score:   9.592, Best Score:  17.217, Critic Loss: 423.2400\n",
      "Episode:   505, Score:   3.588, Best Score:  17.217, Critic Loss: 220.1595\n",
      "Episode:   510, Score:   3.803, Best Score:  17.217, Critic Loss: 293.6435\n",
      "Episode:   515, Score:   3.654, Best Score:  17.217, Critic Loss: 176.0969\n",
      "Episode:   520, Score:   0.934, Best Score:  17.217, Critic Loss: 184.6702\n",
      "Episode:   525, Score:   3.369, Best Score:  17.217, Critic Loss: 352.7697\n",
      "Episode:   530, Score:   4.041, Best Score:  17.217, Critic Loss: 262.2435\n",
      "Episode:   535, Score:   0.786, Best Score:  17.217, Critic Loss: 156.5623\n",
      "Episode:   540, Score:   0.823, Best Score:  17.217, Critic Loss: 139.1952\n",
      "Episode:   545, Score:   1.012, Best Score:  17.217, Critic Loss: 274.8597\n",
      "Episode:   550, Score:   1.063, Best Score:  17.217, Critic Loss: 130.4027\n",
      "Episode:   555, Score:   0.502, Best Score:  17.217, Critic Loss: 14267.4746\n",
      "Episode:   560, Score:   0.728, Best Score:  17.217, Critic Loss: 75675.3281\n",
      "Episode:   565, Score:   0.502, Best Score:  17.217, Critic Loss: 1060.1631\n",
      "Episode:   570, Score:   0.502, Best Score:  17.217, Critic Loss: 1936.1147\n",
      "Episode:   575, Score:   0.502, Best Score:  17.217, Critic Loss: 1332.9663\n",
      "Episode:   580, Score:   0.502, Best Score:  17.217, Critic Loss: 783.3431\n",
      "Episode:   585, Score:   0.502, Best Score:  17.217, Critic Loss: 2353.5801\n",
      "Episode:   590, Score:   0.502, Best Score:  17.217, Critic Loss: 3359.5210\n",
      "Episode:   595, Score:   0.502, Best Score:  17.217, Critic Loss: 795.8097\n",
      "Episode:   600, Score:   0.502, Best Score:  17.217, Critic Loss: 3075.3313\n",
      "Episode:   605, Score:   0.515, Best Score:  17.217, Critic Loss: 3669.3069\n",
      "Episode:   610, Score:   0.502, Best Score:  17.217, Critic Loss: 3840.2510\n",
      "Episode:   615, Score:   0.857, Best Score:  17.217, Critic Loss: 2208.7847\n",
      "Episode:   620, Score:   0.905, Best Score:  17.217, Critic Loss: 1214.9293\n",
      "Episode:   625, Score:   0.852, Best Score:  17.217, Critic Loss: 3838.8391\n",
      "Episode:   630, Score:   0.807, Best Score:  17.217, Critic Loss: 5001.0742\n",
      "Episode:   635, Score:   0.807, Best Score:  17.217, Critic Loss: 2978.4238\n",
      "Episode:   640, Score:   0.502, Best Score:  17.217, Critic Loss: 4689.1709\n",
      "Episode:   645, Score:   0.502, Best Score:  17.217, Critic Loss: 1945.3466\n",
      "Episode:   650, Score:   0.718, Best Score:  17.217, Critic Loss: 5666.9922\n",
      "Episode:   655, Score:   0.502, Best Score:  17.217, Critic Loss: 6503.2998\n",
      "Episode:   660, Score:   0.502, Best Score:  17.217, Critic Loss: 4885.8770\n",
      "Episode:   665, Score:   0.502, Best Score:  17.217, Critic Loss: 4918.3931\n",
      "Episode:   670, Score:   0.679, Best Score:  17.217, Critic Loss: 4508.8745\n",
      "Episode:   675, Score:   2.293, Best Score:  17.217, Critic Loss: 50809.7734\n",
      "Episode:   680, Score:   3.715, Best Score:  17.217, Critic Loss: 42777.7930\n",
      "Episode:   685, Score:   1.156, Best Score:  17.217, Critic Loss: 105891.3516\n",
      "Episode:   690, Score:   2.223, Best Score:  17.217, Critic Loss: 21498.7793\n",
      "Episode:   695, Score:   1.798, Best Score:  17.217, Critic Loss: 22749.0078\n",
      "Episode:   700, Score:   2.211, Best Score:  17.217, Critic Loss: 14829.5186\n",
      "Episode:   705, Score:   2.100, Best Score:  17.217, Critic Loss: 24379.1426\n",
      "Episode:   710, Score:   1.807, Best Score:  17.217, Critic Loss: 8759.6875\n",
      "Episode:   715, Score:   2.759, Best Score:  17.217, Critic Loss: 6383.2656\n",
      "Episode:   720, Score:   2.650, Best Score:  17.217, Critic Loss: 72901.7031\n",
      "Episode:   725, Score:   2.472, Best Score:  17.217, Critic Loss: 98154.8594\n",
      "Episode:   730, Score:   2.490, Best Score:  17.217, Critic Loss: 134526.2188\n",
      "Episode:   735, Score:   2.418, Best Score:  17.217, Critic Loss: 59032.7148\n",
      "Episode:   740, Score:   2.412, Best Score:  17.217, Critic Loss: 235865.2500\n",
      "Episode:   745, Score:   2.270, Best Score:  17.217, Critic Loss: 212450.2812\n",
      "Episode:   750, Score:   2.471, Best Score:  17.217, Critic Loss: 205885.5938\n",
      "Episode:   755, Score:   2.439, Best Score:  17.217, Critic Loss: 151554.1562\n",
      "Episode:   760, Score:   2.482, Best Score:  17.217, Critic Loss: 486108.6562\n",
      "Episode:   765, Score:   2.482, Best Score:  17.217, Critic Loss: 536882.3750\n",
      "Episode:   770, Score:   2.385, Best Score:  17.217, Critic Loss: 317139.5000\n",
      "Episode:   775, Score:   2.046, Best Score:  17.217, Critic Loss: 412930.0312\n",
      "Episode:   780, Score:   9.202, Best Score:  17.217, Critic Loss: 549786.6875\n",
      "Episode:   785, Score:   0.995, Best Score:  17.217, Critic Loss: 372100.6250\n",
      "Episode:   790, Score:   5.432, Best Score:  17.217, Critic Loss: 151490.2031\n",
      "Episode:   795, Score:   1.862, Best Score:  17.217, Critic Loss: 79756.1562\n",
      "Episode:   800, Score:   3.965, Best Score:  17.217, Critic Loss: 242705.7812\n",
      "Episode:   805, Score:   1.005, Best Score:  17.217, Critic Loss: 216842.6875\n",
      "Episode:   810, Score:   8.481, Best Score:  17.217, Critic Loss: 99408.8750\n",
      "Episode:   815, Score:   4.999, Best Score:  17.217, Critic Loss: 119099.3438\n",
      "Episode:   820, Score:   4.748, Best Score:  17.217, Critic Loss: 76119.1875\n",
      "Episode:   825, Score:   9.734, Best Score:  17.217, Critic Loss: 96724.8438\n",
      "Episode:   830, Score:   0.611, Best Score:  17.217, Critic Loss: 129993.4062\n",
      "Episode:   835, Score:   0.502, Best Score:  17.217, Critic Loss: 45446.3750\n",
      "Episode:   840, Score:   0.502, Best Score:  17.217, Critic Loss: 136633.1562\n",
      "Episode:   845, Score:   0.502, Best Score:  17.217, Critic Loss: 48169.7578\n",
      "Episode:   850, Score:   0.502, Best Score:  17.217, Critic Loss: 98844.0312\n",
      "Episode:   855, Score:   0.502, Best Score:  17.217, Critic Loss: 114325.7266\n",
      "Episode:   860, Score:   0.502, Best Score:  17.217, Critic Loss: 107674.4922\n",
      "Episode:   865, Score:   0.502, Best Score:  17.217, Critic Loss: 37041.1094\n",
      "Episode:   870, Score:   0.502, Best Score:  17.217, Critic Loss: 138065.8750\n",
      "Episode:   875, Score:   0.502, Best Score:  17.217, Critic Loss: 64241.6484\n",
      "Episode:   880, Score:   0.502, Best Score:  17.217, Critic Loss: 44953.6406\n",
      "Episode:   885, Score:   0.502, Best Score:  17.217, Critic Loss: 56739.8086\n",
      "Episode:   890, Score:   0.502, Best Score:  17.217, Critic Loss: 63878.2812\n",
      "Episode:   895, Score:   0.502, Best Score:  17.217, Critic Loss: 38826.5156\n",
      "Episode:   900, Score:   0.502, Best Score:  17.217, Critic Loss: 48713.3281\n",
      "Episode:   905, Score:   0.502, Best Score:  17.217, Critic Loss: 74842.3516\n",
      "Episode:   910, Score:   0.502, Best Score:  17.217, Critic Loss: 26763.0156\n",
      "Episode:   915, Score:   0.502, Best Score:  17.217, Critic Loss: 45353.0859\n",
      "Episode:   920, Score:   0.502, Best Score:  17.217, Critic Loss: 58031.7148\n",
      "Episode:   925, Score:   0.502, Best Score:  17.217, Critic Loss: 46528.1797\n",
      "Episode:   930, Score:   0.502, Best Score:  17.217, Critic Loss: 70066.3594\n",
      "Episode:   935, Score:   0.502, Best Score:  17.217, Critic Loss: 903759.1875\n",
      "Episode:   940, Score:   0.502, Best Score:  17.217, Critic Loss: 146096.1719\n",
      "Episode:   945, Score:   0.502, Best Score:  17.217, Critic Loss: 23898.5820\n",
      "Episode:   950, Score:   0.502, Best Score:  17.217, Critic Loss: 32541.0371\n",
      "Episode:   955, Score:   0.502, Best Score:  17.217, Critic Loss: 50476.0000\n",
      "Episode:   960, Score:   0.502, Best Score:  17.217, Critic Loss: 50081.9883\n",
      "Episode:   965, Score:   0.502, Best Score:  17.217, Critic Loss: 84506.3750\n",
      "Episode:   970, Score:   0.502, Best Score:  17.217, Critic Loss: 104687.8125\n",
      "Episode:   975, Score:   0.502, Best Score:  17.217, Critic Loss: 36024.1914\n",
      "Episode:   980, Score:   0.502, Best Score:  17.217, Critic Loss: 1326639.6250\n",
      "Episode:   985, Score:   0.502, Best Score:  17.217, Critic Loss: 35916.8320\n",
      "Episode:   990, Score:   0.502, Best Score:  17.217, Critic Loss: 332191.4375\n",
      "Episode:   995, Score:   0.502, Best Score:  17.217, Critic Loss: 54460.6562\n",
      "Episode:  1000, Score:   0.502, Best Score:  17.217, Critic Loss: 51449.2148\n",
      "Episode:  1005, Score:   0.502, Best Score:  17.217, Critic Loss: 306919.5000\n",
      "Episode:  1010, Score:   0.502, Best Score:  17.217, Critic Loss: 20997.0859\n",
      "Episode:  1015, Score:   0.502, Best Score:  17.217, Critic Loss: 274628.8750\n",
      "Episode:  1020, Score:   0.502, Best Score:  17.217, Critic Loss: 18921.2051\n",
      "Episode:  1025, Score:   0.502, Best Score:  17.217, Critic Loss: 15967.4111\n",
      "Episode:  1030, Score:   0.502, Best Score:  17.217, Critic Loss: 16955.4453\n",
      "Episode:  1035, Score:   0.502, Best Score:  17.217, Critic Loss: 1373073.8750\n",
      "Episode:  1040, Score:   0.502, Best Score:  17.217, Critic Loss: 17911.6172\n",
      "Episode:  1045, Score:   0.502, Best Score:  17.217, Critic Loss: 37801.4336\n",
      "Episode:  1050, Score:   0.502, Best Score:  17.217, Critic Loss: 20157.2988\n",
      "Episode:  1055, Score:   0.502, Best Score:  17.217, Critic Loss: 17850.1816\n",
      "Episode:  1060, Score:   0.326, Best Score:  17.217, Critic Loss: 710816.1875\n",
      "Episode:  1065, Score:   6.891, Best Score:  17.217, Critic Loss: 128534.7656\n",
      "Episode:  1070, Score:   2.227, Best Score:  17.217, Critic Loss: 1712090.0000\n",
      "Episode:  1075, Score:  10.233, Best Score:  17.217, Critic Loss: 187582.3125\n",
      "Episode:  1080, Score:   1.077, Best Score:  17.217, Critic Loss: 192219.6562\n",
      "Episode:  1085, Score:   3.625, Best Score:  17.217, Critic Loss: 388852.0000\n",
      "Episode:  1090, Score:   3.891, Best Score:  17.217, Critic Loss: 91210.9141\n",
      "Episode:  1095, Score:   0.979, Best Score:  17.217, Critic Loss: 168688.9531\n",
      "Episode:  1100, Score:   0.502, Best Score:  17.217, Critic Loss: 299987.7188\n",
      "Episode:  1105, Score:   0.816, Best Score:  17.217, Critic Loss: 341382.6875\n",
      "Episode:  1110, Score:   0.819, Best Score:  17.217, Critic Loss: 516441.3125\n",
      "Episode:  1115, Score:   0.755, Best Score:  17.217, Critic Loss: 4973759.5000\n",
      "Episode:  1120, Score:   2.199, Best Score:  17.217, Critic Loss: 562438.5625\n",
      "Episode:  1125, Score:   2.297, Best Score:  17.217, Critic Loss: 1121619.8750\n",
      "Episode:  1130, Score:   2.436, Best Score:  17.217, Critic Loss: 3590898.5000\n",
      "Episode:  1135, Score:   1.233, Best Score:  17.217, Critic Loss: 2087365.8750\n",
      "Episode:  1140, Score:   2.337, Best Score:  17.217, Critic Loss: 146234.1562\n",
      "Episode:  1145, Score:   1.744, Best Score:  17.217, Critic Loss: 726311.7500\n",
      "Episode:  1150, Score:   2.447, Best Score:  17.217, Critic Loss: 111604.1719\n",
      "Episode:  1155, Score:   2.377, Best Score:  18.553, Critic Loss: 51042.0664\n",
      "Episode:  1160, Score:   2.471, Best Score:  19.055, Critic Loss: 61260.6094\n",
      "Episode:  1165, Score:   2.481, Best Score:  19.055, Critic Loss: 1491059.5000\n",
      "Episode:  1170, Score:   8.495, Best Score:  19.055, Critic Loss: 127433.0625\n",
      "Episode:  1175, Score:   2.318, Best Score:  19.055, Critic Loss: 144145.8125\n",
      "Episode:  1180, Score:   2.466, Best Score:  19.055, Critic Loss: 1259412.3750\n",
      "Episode:  1185, Score:   2.470, Best Score:  19.055, Critic Loss: 53925.3789\n",
      "Episode:  1190, Score:   2.465, Best Score:  19.055, Critic Loss: 133369.1719\n",
      "Episode:  1195, Score:   2.345, Best Score:  19.055, Critic Loss: 26620.4199\n",
      "Episode:  1200, Score:   2.447, Best Score:  19.055, Critic Loss: 9014410.0000\n",
      "Episode:  1205, Score:   2.204, Best Score:  19.055, Critic Loss: 155330.6562\n",
      "Episode:  1210, Score:   2.362, Best Score:  19.055, Critic Loss: 430319.5625\n",
      "Episode:  1215, Score:   2.471, Best Score:  19.055, Critic Loss: 112818.7656\n",
      "Episode:  1220, Score:   2.431, Best Score:  19.055, Critic Loss: 45140.5586\n",
      "Episode:  1225, Score:   2.121, Best Score:  19.055, Critic Loss: 95190.0156\n",
      "Episode:  1230, Score:   2.315, Best Score:  19.055, Critic Loss: 58762.4492\n",
      "Episode:  1235, Score:   2.496, Best Score:  19.055, Critic Loss: 1120327.8750\n",
      "Episode:  1240, Score:   2.108, Best Score:  19.055, Critic Loss: 83069.9453\n",
      "Episode:  1245, Score:   2.287, Best Score:  19.055, Critic Loss: 188967.1562\n",
      "Episode:  1250, Score:   2.486, Best Score:  19.055, Critic Loss: 208569.5000\n",
      "Episode:  1255, Score:   3.079, Best Score:  19.055, Critic Loss: 133418.0000\n",
      "Episode:  1260, Score:   2.411, Best Score:  19.055, Critic Loss: 21124556.0000\n",
      "Episode:  1265, Score:   2.336, Best Score:  19.055, Critic Loss: 75441.4375\n",
      "Episode:  1270, Score:   2.488, Best Score:  19.055, Critic Loss: 1346176.2500\n",
      "Episode:  1275, Score:   2.462, Best Score:  19.055, Critic Loss: 1081606.2500\n",
      "Episode:  1280, Score:   2.454, Best Score:  19.055, Critic Loss: 120057.9531\n",
      "Episode:  1285, Score:   2.215, Best Score:  19.055, Critic Loss: 55142.2305\n",
      "Episode:  1290, Score:   1.539, Best Score:  19.055, Critic Loss: 348119.0625\n",
      "Episode:  1295, Score:   1.919, Best Score:  19.055, Critic Loss: 200103.3750\n",
      "Episode:  1300, Score:   2.005, Best Score:  19.055, Critic Loss: 58153.9141\n",
      "Episode:  1305, Score:   1.371, Best Score:  19.055, Critic Loss: 39795.3438\n",
      "Episode:  1310, Score:   0.945, Best Score:  19.055, Critic Loss: 340059.7188\n",
      "Episode:  1315, Score:   0.575, Best Score:  19.055, Critic Loss: 37206.8711\n",
      "Episode:  1320, Score:   1.335, Best Score:  19.055, Critic Loss: 63560.6953\n",
      "Episode:  1325, Score:   1.755, Best Score:  19.055, Critic Loss: 1586957.3750\n",
      "Episode:  1330, Score:   0.803, Best Score:  19.055, Critic Loss: 50684.8438\n",
      "Episode:  1335, Score:   0.764, Best Score:  19.055, Critic Loss: 1298701.8750\n",
      "Episode:  1340, Score:   0.668, Best Score:  19.055, Critic Loss: 42337.0234\n",
      "Episode:  1345, Score:   0.653, Best Score:  19.055, Critic Loss: 76394.6484\n",
      "Episode:  1350, Score:   1.115, Best Score:  19.055, Critic Loss: 705907.0000\n",
      "Episode:  1355, Score:   0.675, Best Score:  19.055, Critic Loss: 211802.5312\n",
      "Episode:  1360, Score:   0.940, Best Score:  19.055, Critic Loss: 7199123.5000\n",
      "Episode:  1365, Score:   0.655, Best Score:  19.055, Critic Loss: 1567945088.0000\n",
      "Episode:  1370, Score:   0.864, Best Score:  19.055, Critic Loss: 68089.0156\n",
      "Episode:  1375, Score:   0.659, Best Score:  19.055, Critic Loss: 2062764.0000\n",
      "Episode:  1380, Score:   2.101, Best Score:  19.055, Critic Loss: 1627523.1250\n",
      "Episode:  1385, Score:   0.648, Best Score:  19.055, Critic Loss: 555131.1250\n",
      "Episode:  1390, Score:   1.142, Best Score:  19.055, Critic Loss: 2076113408.0000\n",
      "Episode:  1395, Score:   0.803, Best Score:  19.055, Critic Loss: 55178.9453\n",
      "Episode:  1400, Score:   0.530, Best Score:  19.055, Critic Loss: 71727.2891\n",
      "Episode:  1405, Score:   0.671, Best Score:  19.055, Critic Loss: 62456.3594\n",
      "Episode:  1410, Score:   1.043, Best Score:  19.055, Critic Loss: 65627.1719\n",
      "Episode:  1415, Score:   2.322, Best Score:  19.055, Critic Loss: 1875281.7500\n",
      "Episode:  1420, Score:   1.745, Best Score:  19.055, Critic Loss: 3402726.0000\n",
      "Episode:  1425, Score:   2.266, Best Score:  19.055, Critic Loss: 5343664.5000\n",
      "Episode:  1430, Score:  12.289, Best Score:  19.055, Critic Loss: 482845.8750\n",
      "Episode:  1435, Score:   1.037, Best Score:  19.055, Critic Loss: 28043168.0000\n",
      "Episode:  1440, Score:   0.731, Best Score:  19.055, Critic Loss: 18831872.0000\n",
      "Episode:  1445, Score:   2.482, Best Score:  22.131, Critic Loss: 14840836.0000\n",
      "Episode:  1450, Score:   2.451, Best Score:  22.131, Critic Loss: 4363775.0000\n",
      "Episode:  1455, Score:   2.160, Best Score:  22.131, Critic Loss: 4981357.0000\n",
      "Episode:  1460, Score:   2.473, Best Score:  22.131, Critic Loss: 1851965.8750\n",
      "Episode:  1465, Score:   2.395, Best Score:  22.131, Critic Loss: 877135.8125\n",
      "Episode:  1470, Score:   2.440, Best Score:  22.131, Critic Loss: 15368214.0000\n",
      "Episode:  1475, Score:   2.249, Best Score:  22.131, Critic Loss: 34998248.0000\n",
      "Episode:  1480, Score:   2.184, Best Score:  22.131, Critic Loss: 51212472.0000\n",
      "Episode:  1485, Score:   2.197, Best Score:  22.131, Critic Loss: 8415511.0000\n",
      "Episode:  1490, Score:   2.130, Best Score:  22.131, Critic Loss: 23092952.0000\n",
      "Episode:  1495, Score:   2.201, Best Score:  22.131, Critic Loss: 22608116.0000\n",
      "Episode:  1500, Score:   2.153, Best Score:  22.131, Critic Loss: 14857914.0000\n",
      "Episode:  1505, Score:   2.244, Best Score:  22.131, Critic Loss: 29773904.0000\n",
      "Episode:  1510, Score:   2.178, Best Score:  22.131, Critic Loss: 60065176.0000\n",
      "Episode:  1515, Score:   2.242, Best Score:  22.131, Critic Loss: 57400412.0000\n",
      "Episode:  1520, Score:   2.233, Best Score:  22.131, Critic Loss: 12079177728.0000\n",
      "Episode:  1525, Score:   2.281, Best Score:  22.131, Critic Loss: 561984448.0000\n",
      "Episode:  1530, Score:   2.185, Best Score:  22.131, Critic Loss: 170346144.0000\n",
      "Episode:  1535, Score:   2.221, Best Score:  22.131, Critic Loss: 18269952.0000\n",
      "Episode:  1540, Score:   2.331, Best Score:  22.131, Critic Loss: 177347600.0000\n",
      "Episode:  1545, Score:   2.175, Best Score:  22.131, Critic Loss: 49357792.0000\n",
      "Episode:  1550, Score:   3.258, Best Score:  22.131, Critic Loss: 12227500.0000\n",
      "Episode:  1555, Score:   3.970, Best Score:  22.131, Critic Loss: 11485818.0000\n",
      "Episode:  1560, Score:   0.830, Best Score:  22.131, Critic Loss: 25259424.0000\n",
      "Episode:  1565, Score:   3.734, Best Score:  22.131, Critic Loss: 71451448.0000\n",
      "Episode:  1570, Score:   1.241, Best Score:  22.131, Critic Loss: 44549840.0000\n",
      "Episode:  1575, Score:   1.198, Best Score:  22.131, Critic Loss: 129854480.0000\n",
      "Episode:  1580, Score:   9.475, Best Score:  22.131, Critic Loss: 6482407.0000\n",
      "Episode:  1585, Score:   4.636, Best Score:  23.215, Critic Loss: 12619597.0000\n",
      "Episode:  1590, Score:   4.088, Best Score:  23.215, Critic Loss: 12535298.0000\n",
      "Episode:  1595, Score:   4.879, Best Score:  23.215, Critic Loss: 9693581.0000\n",
      "Episode:  1600, Score:   2.515, Best Score:  23.215, Critic Loss: 3871179264.0000\n",
      "Episode:  1605, Score:   0.686, Best Score:  23.215, Critic Loss: 27331604.0000\n",
      "Episode:  1610, Score:   0.734, Best Score:  23.215, Critic Loss: 17375390.0000\n",
      "Episode:  1615, Score:   2.213, Best Score:  23.215, Critic Loss: 236703936.0000\n",
      "Episode:  1620, Score:   2.357, Best Score:  23.215, Critic Loss: 308953792.0000\n",
      "Episode:  1625, Score:   2.249, Best Score:  23.215, Critic Loss: 168020320.0000\n",
      "Episode:  1630, Score:   2.284, Best Score:  23.215, Critic Loss: 2701922560.0000\n",
      "Episode:  1635, Score:   2.254, Best Score:  23.215, Critic Loss: 174635296.0000\n",
      "Episode:  1640, Score:   2.195, Best Score:  23.215, Critic Loss: 104340000.0000\n",
      "Episode:  1645, Score:   2.230, Best Score:  23.215, Critic Loss: 23169384448.0000\n",
      "Episode:  1650, Score:   2.097, Best Score:  23.215, Critic Loss: 115011144.0000\n",
      "Episode:  1655, Score:   2.125, Best Score:  23.215, Critic Loss: 86426112.0000\n",
      "Episode:  1660, Score:   2.307, Best Score:  23.215, Critic Loss: 79305888.0000\n",
      "Episode:  1665, Score:   5.902, Best Score:  23.215, Critic Loss: 237246768.0000\n",
      "Episode:  1670, Score:   1.993, Best Score:  23.215, Critic Loss: 29788334.0000\n",
      "Episode:  1675, Score:   2.224, Best Score:  23.215, Critic Loss: 74781784.0000\n",
      "Episode:  1680, Score:   3.804, Best Score:  23.215, Critic Loss: 23751438.0000\n",
      "Episode:  1685, Score:   2.217, Best Score:  23.215, Critic Loss: 186202640.0000\n",
      "Episode:  1690, Score:   2.314, Best Score:  23.215, Critic Loss: 160196384.0000\n",
      "Episode:  1695, Score:   2.277, Best Score:  23.215, Critic Loss: 188178944.0000\n",
      "Episode:  1700, Score:   2.290, Best Score:  23.215, Critic Loss: 287952768.0000\n",
      "Episode:  1705, Score:   2.259, Best Score:  23.215, Critic Loss: 373336192.0000\n",
      "Episode:  1710, Score:   2.069, Best Score:  23.215, Critic Loss: 259120864.0000\n",
      "Episode:  1715, Score:   2.140, Best Score:  23.215, Critic Loss: 218994752.0000\n",
      "Episode:  1720, Score:   2.340, Best Score:  23.215, Critic Loss: 977503808.0000\n",
      "Episode:  1725, Score:   2.243, Best Score:  23.215, Critic Loss: 365899168.0000\n",
      "Episode:  1730, Score:   2.229, Best Score:  23.215, Critic Loss: 21884905193472.0000\n",
      "Episode:  1735, Score:   2.216, Best Score:  23.215, Critic Loss: 298744128.0000\n",
      "Episode:  1740, Score:   2.157, Best Score:  23.215, Critic Loss: 363152960.0000\n",
      "Episode:  1745, Score:   2.118, Best Score:  23.215, Critic Loss: 368958272.0000\n",
      "Episode:  1750, Score:   2.389, Best Score:  23.215, Critic Loss: 913829824.0000\n",
      "Episode:  1755, Score:   2.252, Best Score:  23.215, Critic Loss: 1254188800.0000\n",
      "Episode:  1760, Score:   2.125, Best Score:  23.215, Critic Loss: 1243780352.0000\n",
      "Episode:  1765, Score:   2.288, Best Score:  23.215, Critic Loss: 280558432.0000\n",
      "Episode:  1770, Score:   2.298, Best Score:  23.215, Critic Loss: 302880128.0000\n",
      "Episode:  1775, Score:   2.243, Best Score:  23.215, Critic Loss: 782620544.0000\n",
      "Episode:  1780, Score:   2.123, Best Score:  23.215, Critic Loss: 656210048.0000\n",
      "Episode:  1785, Score:   2.156, Best Score:  23.215, Critic Loss: 2671856896.0000\n",
      "Episode:  1790, Score:   2.199, Best Score:  23.215, Critic Loss: 499140672.0000\n",
      "Episode:  1795, Score:   2.178, Best Score:  23.215, Critic Loss: 634868608.0000\n",
      "Episode:  1800, Score:   1.508, Best Score:  23.215, Critic Loss: 2209010944.0000\n",
      "Episode:  1805, Score:   2.188, Best Score:  23.215, Critic Loss: 5023993856.0000\n",
      "Episode:  1810, Score:   2.257, Best Score:  23.215, Critic Loss: 3041701888.0000\n",
      "Episode:  1815, Score:   2.368, Best Score:  23.215, Critic Loss: 1084634112.0000\n",
      "Episode:  1820, Score:   2.873, Best Score:  23.215, Critic Loss: 1595976960.0000\n",
      "Episode:  1825, Score:   2.282, Best Score:  23.215, Critic Loss: 2453783040.0000\n",
      "Episode:  1830, Score:   2.210, Best Score:  23.215, Critic Loss: 902321472.0000\n",
      "Episode:  1835, Score:   2.274, Best Score:  23.215, Critic Loss: 16251127808.0000\n",
      "Episode:  1840, Score:   0.657, Best Score:  23.215, Critic Loss: 369225536.0000\n",
      "Episode:  1845, Score:   2.386, Best Score:  23.215, Critic Loss: 698434048.0000\n",
      "Episode:  1850, Score:   1.533, Best Score:  23.215, Critic Loss: 112115761152.0000\n",
      "Episode:  1855, Score:   0.882, Best Score:  23.215, Critic Loss: 796564736.0000\n",
      "Episode:  1860, Score:   0.502, Best Score:  23.215, Critic Loss: 182653232.0000\n",
      "Episode:  1865, Score:   0.628, Best Score:  23.215, Critic Loss: 276623680.0000\n",
      "Episode:  1870, Score:   1.737, Best Score:  23.215, Critic Loss: 639372288.0000\n",
      "Episode:  1875, Score:   2.219, Best Score:  23.215, Critic Loss: 583218240.0000\n",
      "Episode:  1880, Score:   0.567, Best Score:  23.215, Critic Loss: 391596544.0000\n",
      "Episode:  1885, Score:   0.676, Best Score:  23.215, Critic Loss: 708317568.0000\n",
      "Episode:  1890, Score:   0.621, Best Score:  23.215, Critic Loss: 2478579968.0000\n",
      "Episode:  1895, Score:   0.690, Best Score:  23.215, Critic Loss: 848829056.0000\n",
      "Episode:  1900, Score:   0.598, Best Score:  23.215, Critic Loss: 494637792.0000\n",
      "Episode:  1905, Score:   2.291, Best Score:  23.215, Critic Loss: 3177647616.0000\n",
      "Episode:  1910, Score:   2.179, Best Score:  23.215, Critic Loss: 9815292928.0000\n",
      "Episode:  1915, Score:   2.129, Best Score:  23.215, Critic Loss: 1166918400.0000\n",
      "Episode:  1920, Score:   4.903, Best Score:  23.215, Critic Loss: 4136886272.0000\n",
      "Episode:  1925, Score:   0.994, Best Score:  23.215, Critic Loss: 482797408.0000\n",
      "Episode:  1930, Score:   1.353, Best Score:  23.215, Critic Loss: 1217051648.0000\n",
      "Episode:  1935, Score:   1.189, Best Score:  23.215, Critic Loss: 33505769472.0000\n",
      "Episode:  1940, Score:   0.942, Best Score:  23.215, Critic Loss: 629019520.0000\n",
      "Episode:  1945, Score:   1.224, Best Score:  23.215, Critic Loss: 2743821312.0000\n",
      "Episode:  1950, Score:   5.548, Best Score:  23.215, Critic Loss: 1972294400.0000\n",
      "Episode:  1955, Score:   0.701, Best Score:  23.215, Critic Loss: 2228907520.0000\n",
      "Episode:  1960, Score:   0.382, Best Score:  23.215, Critic Loss: 2089091328.0000\n",
      "Episode:  1965, Score:   1.921, Best Score:  23.215, Critic Loss: 2873314304.0000\n",
      "Episode:  1970, Score:   2.069, Best Score:  23.215, Critic Loss: 1646550144.0000\n",
      "Episode:  1975, Score:   1.271, Best Score:  23.215, Critic Loss: 24624484352.0000\n",
      "Episode:  1980, Score:   2.194, Best Score:  23.215, Critic Loss: 30175332925440.0000\n",
      "Episode:  1985, Score:   2.310, Best Score:  23.215, Critic Loss: 2061554048.0000\n",
      "Episode:  1990, Score:   0.391, Best Score:  23.215, Critic Loss: 1652861184.0000\n",
      "Episode:  1995, Score:   3.457, Best Score:  23.215, Critic Loss: 2083846016.0000\n",
      "Episode:  2000, Score:   2.242, Best Score:  23.215, Critic Loss: 3831896320.0000\n"
     ]
    }
   ],
   "source": [
    "# Training Algo\n",
    "\n",
    "total_rewards = []\n",
    "episode_print_step = 5\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "\n",
    "for i_episode in range(1, num_episodes+1):\n",
    "    state = agent.reset_episode()\n",
    "    #print('Initial State: ', state)\n",
    "    episode_reward = 0\n",
    "    x_sub = []\n",
    "    y_sub = []\n",
    "    z_sub = []\n",
    "    while True:\n",
    "        #print('Episode: ', str(i_episode))\n",
    "        #print('State: ', state)\n",
    "        st_store = states_scaler.inverse_transform([state])[0]\n",
    "        x_sub.append(st_store[0])\n",
    "        y_sub.append(st_store[1])\n",
    "        z_sub.append(st_store[2])\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done = task.step(action)\n",
    "        episode_reward += reward\n",
    "        agent.step(action, reward, next_state, done)\n",
    "        \n",
    "        state = states_scaler.transform([next_state])[0]\n",
    "        #print('Next State: ', next_state)\n",
    "        if done:\n",
    "            break\n",
    "    total_rewards.append(episode_reward)\n",
    "    x.append(x_sub)\n",
    "    y.append(y_sub)\n",
    "    z.append(z_sub)\n",
    "    \n",
    "    if (i_episode) % episode_print_step == 0:\n",
    "        print ('Episode: {:5d}, Score: {:7.3f}, Best Score: {:7.3f}, Critic Loss: {:2.4f}'\n",
    "               .format( i_episode, episode_reward, max(total_rewards), agent.critic_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores over the last 10 episodes is: 2.095\n"
     ]
    }
   ],
   "source": [
    "avg_final_scores = np.mean(total_rewards[:-10])\n",
    "print('Average scores over the last 10 episodes is: {:2.3f}'.format(avg_final_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff409744a90>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot paths\n",
    "for i in range(300, 1500, 250):\n",
    "    plt.scatter(x = x[i], y = y[i])\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plot the Rewards\n",
    "\n",
    "Once you are satisfied with your performance, plot the episode rewards, either from a single run, or averaged over multiple runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Plot the rewards.\n",
    "plt.plot(total_rewards)\n",
    "plt.xlabel(s = 'Episode')\n",
    "plt.ylabel(s = 'Rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reflections\n",
    "\n",
    "**Question 1**: Describe the task that you specified in `task.py`.  How did you design the reward function?\n",
    "\n",
    "**Answer**:\n",
    "I have decided to reach a target position from a given initial position. I have assumed that z = 0 is ground level and hitting this will mean a crash. I did several iterations of the reward function.\n",
    "I initially started with the reward function in the file originally that calculated the absolute sum of the difference in simulated and target positions, scaled it and subtracted it from a scalar value (1.0). I soon abandoned this as it was possible for the agent to move away from the target (radial distance, measured as square root of x^2 + y^2 + z^2) but still have no change in reward. For example, x and y could each change by 0.5 but z could change by 1.\n",
    "\n",
    "I decided to change the reward to the radial distance from the target, calculated as -0.02 * radial distance. Here I realised that reward was always negative and so the agent wouldn't generalize.\n",
    "\n",
    "I then changed it to 0.02 * exp(-radial distance). This incentivizes the agent with greater positive reward for reducing the radial distance but I didn't see much difference in the convergence of the learning algorithm and so removed this replacing with a different reward. \n",
    "\n",
    "I decided to go back to the basics and realised that the key think I want is for the agent to go from initial to target position in a straight line and as fast as possible. To achieve this, I create a distance vector to represent target position minus simulated position (x, y and z only). I calculate the cosine value of the angle between the distance vector and simulated velocity vector. The cosine value is 1 when the two vectors are parallel, which is the desired behaviour. The value is 0 when the vectors are orthogonal and it is -1 if the vectors are in opposite direction. Cosine is a non linear function and cosine of 45 degrees is 0.7 and so this offers a nice high value from smaller angles. This is a simple metric and the reward has 2 elements - scalar multiplied by the cosine value and scalar multiplied by max of cosine value and 0.5. This latter term is to give an extra reward for having the right direction. \n",
    "\n",
    "I then added a penalty for a crash by checking if the current z position in the simulated position vector was less than 0. Later I changed this realising that due to the action_repeat loop, it was possible to penalise multiple times. The penalty is now paid when the previous z postion is greater than or equal to zero but goes below zero in the simulated vector. I also force the episode to end by changing done to True. \n",
    "\n",
    "I have added a big positive reward for getting within a 0.5 radial distance of the target. I also force the episode to end.\n",
    "\n",
    "So in the end, I have 3 reward terms - the cosine value (for each step), the reward for reaching target and the penalty in case of crash.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**: Discuss your agent briefly, using the following questions as a guide:\n",
    "\n",
    "- What learning algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**Answer**:\n",
    "I have implemented the DDPG Agent using the notes provided in the Project details. This is implemented in the agent.py file. I have implemented 5 classes - agent, actor, critic, replay_memory and the noise. I have also printed the model summary for the actor's and critic's local and target neural nets.\n",
    "\n",
    "The rewards are linked to the simulated positions (x, y and z) and the velocity vector along x, y and z. Hence, I have modified the state in task.py to be [x, y, z, velocity_x, velocity_y, velocity_z] for each simulation.\n",
    "\n",
    "The description of the 5 classes in agent.py file are:\n",
    "\n",
    "Agent - The hyperparameters used are:\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "tau = 0.05\n",
    "\n",
    "I have made some changes to the code given in the notes. All the hyperparameters are provided as the input to the initialisation of the Agent class. They hyperparameters can be changed through this notebook. The agent, apart from the task (environment), is the only class whose object is created in the notebook. The Agent class interacts with the other 4 classes through initialising the class objects, to building models and running training. The learning algorithm to manage the flow of data and gradients between actor and critic models, along with their local and target models, is managed within the step() function of the Agent class. I haved added floor and cap to the action value in act() to account for very high or very low values due to noise. I have also created a noise_sample for debugging the noise values.\n",
    "\n",
    "Actor - This class has the model design to build the neural nets for actor's local and target models. The hyperparameters are:\n",
    "\n",
    "Learning Rate = 0.005\n",
    "\n",
    "DropOut Rate = 0.1\n",
    "\n",
    "I also provided a list with the number of nodes in the hidden layers - [32,64,32]. This is used to easily change the number of hidden layers as well as the number of nodes in each respective layer from the notebook. I have added Dropout layers and Batch Normalization layers. I have added Batch Normalization to the input layer too as these values have not been preprocessed to fit a particular range and the actual state values can conceptually change significantly. I have used Leaky ReLU as the activation function for the hidden layers with alpha = 0.1. I see the feedback method between the actor and critic in the DDPG architecture as intuitively similar to the Generative Adversarial Models. Using that intuition, I think it is better to use Leaky ReLU to train the model. I have used the tanh activation layer for the output (raw_actions), in line with the DDPG paper and changed the formula for calculating actions. \n",
    "\n",
    "Critic - This class has the model design to build the neural nets for critic's local and target models. The hyperparameters are:\n",
    "\n",
    "Learning Rate = 0.005\n",
    "\n",
    "DropOut Rate = 0.1\n",
    "\n",
    "I also provided a list with the number of nodes in the hidden layers - [32,64], just like the Actor network. I have added Dropout layers and Batch Normalization layers and created a new function to create the nets needed for states and actions. I have used the Duel Network architecture in line with the DDPG paper (and code with the Project). I have used ReLU activation layer for the hidden layers and no activation function for the output (Q-Value). Similar to the Actor model, I have Batch Normalization to the input layers as well.\n",
    "\n",
    "Replay Memory - This class builds a fixed size buffer to store experience tuples. It also provides functions to add experiences and get a sample of experiences. The hyperparameters are:\n",
    "\n",
    "buffer_size = 100000\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "Noise - This is the Ornstein - Uhlenbeck process implementation to add noise to the actions generated by the actor's local model. This noise is useful for exploration to find better policies. The hyperparameters are:\n",
    "\n",
    "noise_mu = 0.0\n",
    "\n",
    "noise_theta = 0.15\n",
    "\n",
    "noise_sigma = 0.2 * 5\n",
    "\n",
    "I decided to have a high sigma as the noise is added to the actions that have a pretty big range from 0 to 900. \n",
    "\n",
    "I tried the architecture with and without Batch Normalization and Dropout layers. I didn't notice major changes to the result from the Dropout layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**Answer**:\n",
    "Once I had read through the paper on DDPG, I was able to follow the theory behind the learning algorithm. I have found the learning process to be very difficult in practice. I have played around with numerous combination of parameters with some success. I have tried very large values for noise through theta and sigma parameters and though, it does help in achieving some very high positive value rewards, it makes the whole learning processing very unstable. I have tried different values for the actor and critic learning rates (0.1 to 0.0001), gamma (0.8 to 0.99) and tau(0.1 to 0.001). I added a scaling factor, 0.95, to the action values in order to avoid the maximum and minimum allowed values of the actions in order to improve learning. I also tried adding a minimum of 10 times the batch size amount of experiences to the replay buffer before I start learning so that there are more samples to choose from. I tried different batch sizes of 128, 256 and 512 but the performance was the same. Changing the number of hidden layers to add one more or increasing the number of units in hidden layer to 2-5 times the current number of units, also did not improve the performance. I switched the activation functions from ReLU to Leaky ReLU and the sigmoid to tanh. Finally I also played with the DropOut alpha (0.1 to 0.3) and changing the number of Batch Normalization layers. I also added Batch Normalization layers to the input layers for actor and critic models as both states and actions values are not preprocessed to fit a defined range of [0,1] or [-1,1] and can move significantly. However, during debugging I found that, despite all attempts, the actor local model would not provide high rotor speed to all 4 rotors even when the z position value was going down (the target position has a higher z position value). In fact, in most of the experiences I observed in debugging, I found that the rotor speed was either almost maximum or the minimum value of the action range. The agent as a result consistent moves down (z position value keeps reducing). This gives a negative reward as the direction of velocity and the direction of travel needed are more than 90 degrees apart. I have found it hard to understand why the model would behave this way and why the learning process wouldn't train the agent to increase the action for all 4 rotors. How do I force the agent to create such experiences? I have tried searching on the Student Hub and Knowledge Hub but couldn't get any help for this problem. \n",
    "\n",
    "Another drawback of the above issue is that since the agent is not able to move in the right direction, it doesn't get close enough to the target position to earn the big positive reward and develop experiences for this. This really exposes the problem of how to train the agent for states it hasn't experienced.\n",
    "\n",
    "I found my main breakthrough moments as I improved on the rewards calculation. The scores became more stable with each attempt and the best scores also improved significantly. In my last iteration, the average rewards over the last 10 episodes was -2.866. I would expect the rewards to be much higher with a well trained agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, plotting, specifying the task, etc.)\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**Answer**:\n",
    "Getting started was particularly hard as I had no prior practical experience of creating such an agent. I spent a lot of time understanding the DDPG paper and also read through some answers to questions on the Knowledge Hub. There are a few links  to StackOverflow, provided on Knowledge Hub (https://knowledge.udacity.com/questions/32745) to help understand the theory. \n",
    "I have found training the agent to be the hardest part. I have tried numerous combinations of the hyperparameters and architectural changes as detailed in Answer 3 but still could not see much improvement in the total rewards. \n",
    "\n",
    "I noticed that the agent was particularly sensitive to the reward function. The values and the calculation method had to be refined so as to avoid big swings in rewards and also keep it simple. I found this part of the exercise to be very interesting.\n",
    "\n",
    "I have some ideas for further improvement. The current implementation provides the states to the agent and trains to reach close to a target position. This training is then specific to the initial position and the final position. Though the reward function mainly relies on the direction of travel to get rewards for individual simulation step, the weights of the neural network models for the actor and critic have been trained for the specific range of the states experienced during training. This is not suitable in practice as we would prefer to train the agent once and then be able to instruct the quadcopter to fly to any target position. There can also be addition of wind in the environment to make this more real-life. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
